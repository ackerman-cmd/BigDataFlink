# Решение

## Задание

Необходимо реализовать потоковую обработку данных с помощью Apache Flink, который читает топик Kafka, трансформирует данные в режиме streaming в модель "звезда" и записывает результат в PostgreSQL. Данные в Kafka-топиках хранятся в формате JSON, а в PostgreSQL — в трансформированном виде.

## 1. Загрузка данных

- Реализована отправка всех строк из файлов `mock_data` построчно (перемещено в сервис `loader_service`).
- 10 файлов обрабатываются параллельно в 10 потоках:
  - Каждая строка читается построчно.
  - Для каждой строки генерируется и устанавливается уникальный идентификатор (для глобальной согласованности данных).
  - Строка преобразуется в JSON и отправляется в Kafka (кластер из 5 партиций для распределённости).

## 2. Loader Service

- `loader_service` — Spring Boot приложение с вышеописанной функциональностью.

## 3. Обработка данных во Flink

- Запускается Flink в кластере.
- Приложение `streaming_processing`:
  - Регистрируется Flink job.
  - Через Flink Kafka Connector подключается к Kafka и считывает данные.
  - Данные преобразуются в схему "снежинка" по DDL (из файла `ddl.sql`).
  - С помощью JDBC API данные вставляются в базу данных.

## 4. Стриминг

- Стриминг работает непрерывно и распределённо в кластере.

---

## Как запустить

Из корня проекта выполните:

```bash
docker-compose up --build -d
```